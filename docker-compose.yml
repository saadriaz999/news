version: '3.9'

services:

  # main backend postgres database
  db:
    image: ankane/pgvector
    container_name: postgres_db
    environment:
      POSTGRES_DB: mydatabase
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # python django backend
  django:
    image: python:3.11
    container_name: django_app
    command: >
      bash -c "python -m pip install --upgrade pip &&
               pip install --no-cache-dir -r requirements.txt &&
               python manage.py migrate &&
               python manage.py runserver 0.0.0.0:8000"
    volumes:
      - ./django-backend:/app
    working_dir: /app
    ports:
      - "8000:8000"
    depends_on:
      - db
    environment:
      - DB_NAME=mydatabase
      - DB_USER=myuser
      - DB_PASSWORD=mypassword
      - DB_HOST=db
      - DB_PORT=5432

  # stream lit UI
  streamlit:
    image: python:3.11
    container_name: streamlit
    working_dir: /app
    volumes:
      - ./streamlit-ui:/app
    ports:
      - "8501:8501"
    depends_on:
      - django
    command: >
      sh -c "python -m pip install --upgrade pip &&
             pip install --no-cache-dir -r requirements.txt &&
             streamlit run streamlit-ui.py --server.port=8501 --server.address=0.0.0.0"

  # kafka dependency
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  # kafka
  kafka:
    image: confluentinc/cp-kafka:7.6.0
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
      - django

  # airflow database
  airflow_postgres:
    image: postgres:13
    container_name: airflow_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow_pg_data:/var/lib/postgresql/data

  # airflow ui
  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow_web
    restart: always
    depends_on:
      - airflow_postgres
      - kafka
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_postgres/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: 'b4007bf7d4591a839a01b7b1bab251f3a5afef03f7797e1373aaafa2c4f0'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    ports:
      - "8080:8080"
    volumes:
      - ./data_pipeline/dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data_pipeline/kafka:/opt/airflow/kafka
      - ./data_pipeline/requirements.txt:/requirements.txt
    command: >
      bash -c "pip install -r /requirements.txt && airflow webserver"

  # airflow backend
  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow_scheduler
    restart: always
    depends_on:
      - airflow_postgres
      - kafka
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./data_pipeline/dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data_pipeline/kafka:/opt/airflow/kafka
      - ./data_pipeline/requirements.txt:/requirements.txt
    command: >
      bash -c "pip install -r /requirements.txt && airflow scheduler"

  # airflow setup
  airflow-init:
    image: apache/airflow:2.8.1
    container_name: airflow_init
    depends_on:
      - airflow_postgres
      - kafka
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_postgres/airflow
    volumes:
      - ./data_pipeline/dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data_pipeline/kafka:/opt/airflow/kafka
      - ./data_pipeline/requirements.txt:/requirements.txt
    entrypoint: /bin/bash
    command: >
      -c "pip install -r /requirements.txt &&
          airflow db init &&
          airflow users create --username admin --password admin --firstname Saad --lastname User --role Admin --email admin@example.com"

  # spark
  spark:
    image: bitnami/spark:3.3
    container_name: spark
    ports:
      - "7077:7077"   # Spark master port
      - "4040:4040"   # Spark UI
    volumes:
      - ./data_pipeline/requirements.txt:/requirements.txt
      - ./data_pipeline/spark:/opt/spark/app
      - ./data_pipeline/spark_files/jars:/opt/spark/jars/hadoop
      - ./data_pipeline/spark/start_spark.sh:/opt/spark/start_spark.sh
    working_dir: /app
    environment:
      - SPARK_MODE=master
    command: [ "bash", "-c", "pip install -r /requirements.txt && bash /opt/spark/start_spark.sh" ]
    depends_on:
      - kafka

volumes:
  postgres_data:
  airflow_pg_data:
